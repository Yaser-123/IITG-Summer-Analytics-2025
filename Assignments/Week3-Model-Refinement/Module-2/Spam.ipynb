{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e8ebd94",
   "metadata": {},
   "source": [
    "# ğŸš€ Spam Detection System using Multinomial Naive Bayes\n",
    "\n",
    "## ğŸ“§ Email Classification Project\n",
    "\n",
    "Welcome to this comprehensive spam detection system! This notebook demonstrates how to build an effective email classifier using machine learning techniques.\n",
    "\n",
    "### ğŸ¯ **Project Overview**\n",
    "- **Goal**: Classify emails as spam ğŸš« or ham âœ…\n",
    "- **Algorithm**: Multinomial Naive Bayes\n",
    "- **Features**: Text vectorization using CountVectorizer\n",
    "- **Pipeline**: Automated preprocessing and prediction\n",
    "\n",
    "### ğŸ“Š **What You'll Learn**\n",
    "1. Data exploration and preprocessing\n",
    "2. Feature extraction from text data\n",
    "3. Model training and evaluation\n",
    "4. Building ML pipelines for production\n",
    "\n",
    "---\n",
    "\n",
    "*Let's dive into the world of email classification!* ğŸ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb55dbc",
   "metadata": {},
   "source": [
    "## ğŸ“š Step 1: Import Required Libraries\n",
    "\n",
    "We'll start by importing the essential Python libraries for our spam detection project:\n",
    "\n",
    "- **pandas** ğŸ¼: For data manipulation and analysis\n",
    "- **scikit-learn** ğŸ§ : For machine learning algorithms and tools\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/KAq5w47R9rmTuvWOWa/giphy.gif\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2de33fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data manipulation and analysis\n",
    "import pandas as pd  # For data loading, manipulation, and analysis\n",
    "import numpy as np   # For numerical operations and array handling\n",
    "\n",
    "# Display settings for better output visualization\n",
    "pd.set_option('display.max_columns', None)  # Show all columns\n",
    "pd.set_option('display.width', None)        # Prevent line wrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e910d4",
   "metadata": {},
   "source": [
    "## ğŸ“„ Step 2: Load and Explore the Dataset\n",
    "\n",
    "Let's load our spam detection dataset and take a first look at the data structure:\n",
    "\n",
    "- **Dataset**: `spam.csv` containing email messages and their labels\n",
    "- **Exploration**: Understanding the data distribution and format\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/3oKIPEqDGUULpEU0aQ/giphy.gif\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2f5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset loaded successfully!\n",
      "ğŸ“Š Dataset shape: (5572, 2)\n",
      "ğŸ“‹ Columns: ['Category', 'Message']\n",
      "\n",
      "ğŸ” First 5 rows of the dataset:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Message",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e0fb4252-fca9-4216-bda9-52db6c07cefa",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat..."
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni..."
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say..."
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the spam detection dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"spam.csv\", encoding='latin-1')  # Handle potential encoding issues\n",
    "    print(f\"âœ… Dataset loaded successfully!\")\n",
    "    print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "    print(f\"ğŸ“‹ Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display first few rows to understand data structure\n",
    "    print(\"\\nğŸ” First 5 rows of the dataset:\")\n",
    "    display(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Error: spam.csv file not found. Please ensure the file is in the correct directory.\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading dataset: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89165c33",
   "metadata": {},
   "source": [
    "## ğŸ” Step 3: Data Analysis and Statistics\n",
    "\n",
    "Time to analyze our dataset! Let's examine the distribution of spam vs ham messages:\n",
    "\n",
    "- **Statistical Analysis**: Understanding data distribution\n",
    "- **Category Breakdown**: Spam vs Ham ratio\n",
    "- **Data Quality Check**: Ensuring clean, usable data\n",
    "\n",
    "ğŸ“Š **Key Metrics to Watch:**\n",
    "- Total messages count\n",
    "- Spam vs Ham distribution\n",
    "- Message length statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a0d12f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š DATASET ANALYSIS REPORT\n",
      "==================================================\n",
      "ğŸ“ˆ Total number of messages: 5572\n",
      "ğŸ“‹ Number of features: 2\n",
      "ğŸ” Data types:\n",
      "Category    object\n",
      "Message     object\n",
      "dtype: object\n",
      "\n",
      "ğŸ” Missing values analysis:\n",
      "Category    0\n",
      "Message     0\n",
      "dtype: int64\n",
      "\n",
      "ğŸ“Š Message category distribution:\n",
      "Category\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ“ˆ Percentage distribution:\n",
      "ham: 86.59%\n",
      "spam: 13.41%\n",
      "\n",
      "ğŸ“‹ Statistical summary by category:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('Message', 'count')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "('Message', 'unique')",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "('Message', 'top')",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "('Message', 'freq')",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "737e0837-7d86-4d88-9ffa-4f8d15ed493b",
       "rows": [
        [
         "ham",
         "4825",
         "4516",
         "Sorry, I'll call later",
         "30"
        ],
        [
         "spam",
         "747",
         "641",
         "Please call our customer service representative on FREEPHONE 0808 145 4742 between 9am-11pm as you have WON a guaranteed Ã‚Â£1000 cash or Ã‚Â£5000 prize!",
         "4"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 2
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">Message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>641</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Message                                                            \\\n",
       "           count unique                                                top   \n",
       "Category                                                                     \n",
       "ham         4825   4516                             Sorry, I'll call later   \n",
       "spam         747    641  Please call our customer service representativ...   \n",
       "\n",
       "               \n",
       "         freq  \n",
       "Category       \n",
       "ham        30  \n",
       "spam        4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comprehensive data analysis and statistics\n",
    "print(\"ğŸ“Š DATASET ANALYSIS REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Basic dataset information\n",
    "print(f\"ğŸ“ˆ Total number of messages: {len(df)}\")\n",
    "print(f\"ğŸ“‹ Number of features: {df.shape[1]}\")\n",
    "print(f\"ğŸ” Data types:\\n{df.dtypes}\\n\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"ğŸ” Missing values analysis:\")\n",
    "missing_data = df.isnull().sum()\n",
    "print(missing_data)\n",
    "print()\n",
    "\n",
    "# Category distribution analysis\n",
    "print(\"ğŸ“Š Message category distribution:\")\n",
    "category_counts = df['Category'].value_counts()\n",
    "print(category_counts)\n",
    "print(f\"\\nğŸ“ˆ Percentage distribution:\")\n",
    "category_percentages = df['Category'].value_counts(normalize=True) * 100\n",
    "for category, percentage in category_percentages.items():\n",
    "    print(f\"{category}: {percentage:.2f}%\")\n",
    "\n",
    "print(\"\\nğŸ“‹ Statistical summary by category:\")\n",
    "# Group by category and provide detailed statistics\n",
    "summary_stats = df.groupby('Category').describe()\n",
    "display(summary_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152eb3f3",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 4: Data Preprocessing\n",
    "\n",
    "Let's prepare our data for machine learning! We need to convert categorical labels to numerical format:\n",
    "\n",
    "- **Label Encoding**: Convert 'spam'/'ham' to 1/0\n",
    "- **Binary Classification**: Setting up for ML algorithms\n",
    "- **Data Transformation**: Making data ML-ready\n",
    "\n",
    "âš¡ **Transformation Process:**\n",
    "- 'spam' â†’ 1 (Positive class)\n",
    "- 'ham' â†’ 0 (Negative class)\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/xT9IgzoKnwFNmISR8I/giphy.gif\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a6551fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ PREPROCESSING: Label Encoding\n",
      "========================================\n",
      "âœ… Label encoding completed!\n",
      "ğŸ“Š Original categories: ['ham' 'spam']\n",
      "ğŸ”¢ Encoded values: [0 1]\n",
      "\n",
      "ğŸ“‹ Label mapping verification:\n",
      "  Category  spam\n",
      "0      ham     0\n",
      "2     spam     1\n",
      "\n",
      "ğŸ“ˆ Class distribution after encoding:\n",
      "Class 0 (Ham): 4825 messages (86.59%)\n",
      "Class 1 (Spam): 747 messages (13.41%)\n",
      "\n",
      "ğŸ” Sample of processed data:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Category",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Message",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "spam",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e2ee4457-2022-493b-994e-69e0af64b648",
       "rows": [
        [
         "0",
         "ham",
         "Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...",
         "0"
        ],
        [
         "1",
         "ham",
         "Ok lar... Joking wif u oni...",
         "0"
        ],
        [
         "2",
         "spam",
         "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's",
         "1"
        ],
        [
         "3",
         "ham",
         "U dun say so early hor... U c already then say...",
         "0"
        ],
        [
         "4",
         "ham",
         "Nah I don't think he goes to usf, he lives around here though",
         "0"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Category                                            Message  spam\n",
       "0      ham  Go until jurong point, crazy.. Available only ...     0\n",
       "1      ham                      Ok lar... Joking wif u oni...     0\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3      ham  U dun say so early hor... U c already then say...     0\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data preprocessing: Convert categorical labels to numerical format\n",
    "print(\"ğŸ”§ PREPROCESSING: Label Encoding\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create binary target variable (0 = ham, 1 = spam)\n",
    "df['spam'] = df['Category'].apply(lambda x: 1 if x == 'spam' else 0)\n",
    "\n",
    "# Verify the encoding\n",
    "print(\"âœ… Label encoding completed!\")\n",
    "print(f\"ğŸ“Š Original categories: {df['Category'].unique()}\")\n",
    "print(f\"ğŸ”¢ Encoded values: {df['spam'].unique()}\")\n",
    "\n",
    "# Verify the mapping is correct\n",
    "label_mapping = df[['Category', 'spam']].drop_duplicates().sort_values('spam')\n",
    "print(f\"\\nğŸ“‹ Label mapping verification:\")\n",
    "print(label_mapping)\n",
    "\n",
    "# Show class distribution after encoding\n",
    "print(f\"\\nğŸ“ˆ Class distribution after encoding:\")\n",
    "spam_distribution = df['spam'].value_counts().sort_index()\n",
    "for label, count in spam_distribution.items():\n",
    "    category_name = 'Ham' if label == 0 else 'Spam'\n",
    "    percentage = (count / len(df)) * 100\n",
    "    print(f\"Class {label} ({category_name}): {count} messages ({percentage:.2f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ” Sample of processed data:\")\n",
    "display(df[['Category', 'Message', 'spam']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c2e3e4",
   "metadata": {},
   "source": [
    "## ğŸ¯ Step 5: Train-Test Data Split\n",
    "\n",
    "Time to split our data for training and testing! This ensures we can properly evaluate our model:\n",
    "\n",
    "- **Training Set**: Used to train the model (75% default)\n",
    "- **Testing Set**: Used to evaluate performance (25% default)\n",
    "- **Features (X)**: Email messages\n",
    "- **Labels (y)**: Spam classification (0/1)\n",
    "\n",
    "ğŸ”„ **Why Split Data?**\n",
    "- Prevents overfitting\n",
    "- Provides unbiased performance evaluation\n",
    "- Simulates real-world scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f4bc27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ TRAIN-TEST DATA SPLITTING\n",
      "===================================\n",
      "âœ… Data split completed successfully!\n",
      "ğŸ“Š Total dataset size: 5572 messages\n",
      "ğŸ‹ï¸  Training set size: 4179 messages (75.0%)\n",
      "ğŸ§ª Testing set size: 1393 messages (25.0%)\n",
      "\n",
      "ğŸ“ˆ Class distribution verification:\n",
      "Training set:\n",
      "  Class 0 (Ham): 86.60%\n",
      "  Class 1 (Spam): 13.40%\n",
      "Testing set:\n",
      "  Class 0 (Ham): 86.58%\n",
      "  Class 1 (Spam): 13.42%\n"
     ]
    }
   ],
   "source": [
    "# Import train-test split functionality\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"ğŸ¯ TRAIN-TEST DATA SPLITTING\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df['Message']  # Email messages (features)\n",
    "y = df['spam']     # Spam labels (target)\n",
    "\n",
    "# Split the data with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.25,        # 25% for testing, 75% for training\n",
    "    random_state=42,       # For reproducible results\n",
    "    stratify=y            # Maintain class distribution in both sets\n",
    ")\n",
    "\n",
    "# Display split information\n",
    "print(f\"âœ… Data split completed successfully!\")\n",
    "print(f\"ğŸ“Š Total dataset size: {len(df)} messages\")\n",
    "print(f\"ğŸ‹ï¸  Training set size: {len(X_train)} messages ({(len(X_train)/len(df)*100):.1f}%)\")\n",
    "print(f\"ğŸ§ª Testing set size: {len(X_test)} messages ({(len(X_test)/len(df)*100):.1f}%)\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(f\"\\nğŸ“ˆ Class distribution verification:\")\n",
    "print(\"Training set:\")\n",
    "train_dist = y_train.value_counts(normalize=True) * 100\n",
    "for label, percentage in train_dist.sort_index().items():\n",
    "    class_name = 'Ham' if label == 0 else 'Spam'\n",
    "    print(f\"  Class {label} ({class_name}): {percentage:.2f}%\")\n",
    "\n",
    "print(\"Testing set:\")\n",
    "test_dist = y_test.value_counts(normalize=True) * 100\n",
    "for label, percentage in test_dist.sort_index().items():\n",
    "    class_name = 'Ham' if label == 0 else 'Spam'\n",
    "    print(f\"  Class {label} ({class_name}): {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5252c9",
   "metadata": {},
   "source": [
    "## ğŸ“ Step 6: Text Vectorization\n",
    "\n",
    "Converting text to numbers! Machines can't understand words directly, so we need to transform text into numerical features:\n",
    "\n",
    "- **CountVectorizer**: Converts text to word count vectors\n",
    "- **Bag of Words**: Creates feature matrix from text\n",
    "- **Tokenization**: Breaks text into individual words\n",
    "\n",
    "ğŸ”¢ **How it Works:**\n",
    "1. Tokenize text into words\n",
    "2. Create vocabulary from all words\n",
    "3. Count word occurrences per document\n",
    "4. Generate numerical feature matrix\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/l46Cy1rHbQ92uuLXa/giphy.gif\" width=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8052ed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ TEXT VECTORIZATION PROCESS\n",
      "========================================\n",
      "ğŸ”„ Fitting vectorizer on training data...\n",
      "âœ… Text vectorization completed!\n",
      "ğŸ“Š Vocabulary size: 5,000 unique words\n",
      "ğŸ“ Training matrix shape: (4179, 5000)\n",
      "ğŸ’¾ Matrix sparsity: 100.00% non-zero values\n",
      "\n",
      "ğŸ” Sample of vectorized data (first 2 documents, first 10 features):\n",
      "âœ… Text vectorization completed!\n",
      "ğŸ“Š Vocabulary size: 5,000 unique words\n",
      "ğŸ“ Training matrix shape: (4179, 5000)\n",
      "ğŸ’¾ Matrix sparsity: 100.00% non-zero values\n",
      "\n",
      "ğŸ” Sample of vectorized data (first 2 documents, first 10 features):\n",
      "Feature names: ['00', '00 sub', '000', '000 bonus', '000 cash', '000 homeowners', '000 pounds', '000 prize', '000 xmas', '01223585334']\n",
      "Document 1 counts: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Document 2 counts: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "ğŸ“ Sample vocabulary words:\n",
      "['tot', 'say', 'dun', 'believe', 'dun believe', 'alex', 'says', 'ok', 'ok ok', 'Ã£Â¼', 'going', 'esplanade', 'fr', 'home', 'Ã£Â¼ going', 'lol', 'yes', 'add', 'day', 'lol yes']\n",
      "Feature names: ['00', '00 sub', '000', '000 bonus', '000 cash', '000 homeowners', '000 pounds', '000 prize', '000 xmas', '01223585334']\n",
      "Document 1 counts: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Document 2 counts: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "ğŸ“ Sample vocabulary words:\n",
      "['tot', 'say', 'dun', 'believe', 'dun believe', 'alex', 'says', 'ok', 'ok ok', 'Ã£Â¼', 'going', 'esplanade', 'fr', 'home', 'Ã£Â¼ going', 'lol', 'yes', 'add', 'day', 'lol yes']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"ğŸ“ TEXT VECTORIZATION PROCESS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Initialize CountVectorizer with optimized parameters\n",
    "vectorizer = CountVectorizer(\n",
    "    lowercase=True,           # Convert all text to lowercase\n",
    "    stop_words='english',     # Remove common English stop words\n",
    "    max_features=5000,        # Limit vocabulary size for efficiency\n",
    "    ngram_range=(1, 2),       # Include both unigrams and bigrams\n",
    "    min_df=2,                 # Ignore terms appearing in less than 2 documents\n",
    "    max_df=0.95              # Ignore terms appearing in more than 95% of documents\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on training data and transform\n",
    "print(\"ğŸ”„ Fitting vectorizer on training data...\")\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train.values)\n",
    "\n",
    "print(f\"âœ… Text vectorization completed!\")\n",
    "print(f\"ğŸ“Š Vocabulary size: {len(vectorizer.vocabulary_):,} unique words\")\n",
    "print(f\"ğŸ“ Training matrix shape: {X_train_vectorized.shape}\")\n",
    "print(f\"ğŸ’¾ Matrix sparsity: {((X_train_vectorized.nnz / X_train_vectorized.size) * 100):.2f}% non-zero values\")\n",
    "\n",
    "# Display sample of the vectorized data\n",
    "print(f\"\\nğŸ” Sample of vectorized data (first 2 documents, first 10 features):\")\n",
    "sample_matrix = X_train_vectorized.toarray()[:2, :10]\n",
    "feature_names = vectorizer.get_feature_names_out()[:10]\n",
    "\n",
    "print(\"Feature names:\", feature_names.tolist())\n",
    "print(\"Document 1 counts:\", sample_matrix[0].tolist())\n",
    "print(\"Document 2 counts:\", sample_matrix[1].tolist())\n",
    "\n",
    "# Show some example words from vocabulary\n",
    "print(f\"\\nğŸ“ Sample vocabulary words:\")\n",
    "sample_words = list(vectorizer.vocabulary_.keys())[:20]\n",
    "print(sample_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d028ae74",
   "metadata": {},
   "source": [
    "## ğŸ¤– Step 7: Model Training - Multinomial Naive Bayes\n",
    "\n",
    "Time to train our spam detector! We're using Multinomial Naive Bayes - perfect for text classification:\n",
    "\n",
    "- **Algorithm**: Multinomial Naive Bayes\n",
    "- **Best For**: Text classification with word count features\n",
    "- **Assumption**: Features are conditionally independent\n",
    "- **Speed**: Fast training and prediction\n",
    "\n",
    "ğŸ§  **Why Naive Bayes for Spam Detection?**\n",
    "- Excellent with text data\n",
    "- Handles high-dimensional sparse features\n",
    "- Robust to irrelevant features\n",
    "- Fast and memory efficient\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/LaVp0AyqR5bGsC5Cbm/giphy.gif\" width=\"400\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab4e1709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¤– MODEL TRAINING - MULTINOMIAL NAIVE BAYES\n",
      "==================================================\n",
      "âš™ï¸  Model configuration:\n",
      "   Algorithm: Multinomial Naive Bayes\n",
      "   Smoothing parameter (alpha): 1.0\n",
      "   Learn class priors: True\n",
      "\n",
      "ğŸ‹ï¸  Training model on 4,179 samples...\n",
      "âœ… Model training completed successfully!\n",
      "â±ï¸  Training time: 0.0063 seconds\n",
      "ğŸ“Š Training samples: 4,179\n",
      "ğŸ“ Feature dimensions: 5,000\n",
      "\n",
      "ğŸ“ˆ Learned class prior probabilities:\n",
      "   Ham (Class 0): 0.8660 (86.60%)\n",
      "   Spam (Class 1): 0.1340 (13.40%)\n",
      "\n",
      "ğŸ¯ Model is ready for making predictions!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ¤– MODEL TRAINING - MULTINOMIAL NAIVE BAYES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Initialize the Multinomial Naive Bayes model\n",
    "# Alpha parameter controls smoothing (default=1.0 for Laplace smoothing)\n",
    "naive_bayes_model = MultinomialNB(\n",
    "    alpha=1.0,        # Laplace smoothing parameter\n",
    "    fit_prior=True    # Learn class prior probabilities\n",
    ")\n",
    "\n",
    "print(\"âš™ï¸  Model configuration:\")\n",
    "print(f\"   Algorithm: Multinomial Naive Bayes\")\n",
    "print(f\"   Smoothing parameter (alpha): {naive_bayes_model.alpha}\")\n",
    "print(f\"   Learn class priors: {naive_bayes_model.fit_prior}\")\n",
    "\n",
    "# Record training time for performance analysis\n",
    "print(f\"\\nğŸ‹ï¸  Training model on {X_train_vectorized.shape[0]:,} samples...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "naive_bayes_model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Model training completed successfully!\")\n",
    "print(f\"â±ï¸  Training time: {training_time:.4f} seconds\")\n",
    "print(f\"ğŸ“Š Training samples: {X_train_vectorized.shape[0]:,}\")\n",
    "print(f\"ğŸ“ Feature dimensions: {X_train_vectorized.shape[1]:,}\")\n",
    "\n",
    "# Display learned class probabilities\n",
    "class_priors = naive_bayes_model.class_log_prior_\n",
    "class_names = ['Ham (Class 0)', 'Spam (Class 1)']\n",
    "print(f\"\\nğŸ“ˆ Learned class prior probabilities:\")\n",
    "for i, (class_name, log_prior) in enumerate(zip(class_names, class_priors)):\n",
    "    prior_prob = np.exp(log_prior)\n",
    "    print(f\"   {class_name}: {prior_prob:.4f} ({prior_prob*100:.2f}%)\")\n",
    "\n",
    "# Model is now ready for predictions\n",
    "print(f\"\\nğŸ¯ Model is ready for making predictions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2d696",
   "metadata": {},
   "source": [
    "## ğŸ”® Step 8: Testing Predictions on Sample Emails\n",
    "\n",
    "Let's test our trained model with some real-world examples! We'll use sample emails to see how well our detector works:\n",
    "\n",
    "- **Sample 1**: Normal friendly message (should be Ham âœ…)\n",
    "- **Sample 2**: Promotional/discount message (might be Spam ğŸš«)\n",
    "\n",
    "ğŸ’¡ **Prediction Process:**\n",
    "1. Transform new emails using the same vectorizer\n",
    "2. Use trained model to predict spam probability\n",
    "3. Get binary classification (0 = Ham, 1 = Spam)\n",
    "\n",
    "- **pandas** ğŸ¼: For data manipulation and analysis  \n",
    "- **scikit-learn** ğŸ§ : For machine learning algorithms and tools  \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExejRveGRqNGI0ZGs0eGFwNnd3empwYno2YWhkb2Q3aHV1ajIydHV6MCZlcD12MV9naWZzX3NlYXJjaCZjdD1n/51AhgeKNAamtcmcpGx/giphy.gif\" width=\"300\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f775881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”® TESTING MODEL PREDICTIONS\n",
      "===================================\n",
      "ğŸ“§ Testing 4 sample emails:\n",
      "--------------------------------------------------\n",
      "\n",
      "ğŸ“¨ Email 1:\n",
      "Content: \"Hey John, can we get together to watch the football game tomorrow? Let me know w...\"\n",
      "Prediction: âœ… HAM\n",
      "Confidence: Ham 100.00% | Spam 0.00%\n",
      "Risk Level: ğŸŸ¢ LOW RISK\n",
      "\n",
      "ğŸ“¨ Email 2:\n",
      "Content: \"URGENT! Up to 50% discount on luxury watches! Exclusive offer just for you. Don'...\"\n",
      "Prediction: ğŸš« SPAM\n",
      "Confidence: Ham 0.73% | Spam 99.27%\n",
      "Risk Level: ğŸ”´ HIGH RISK\n",
      "\n",
      "ğŸ“¨ Email 3:\n",
      "Content: \"Hi Mom, thanks for the birthday wishes. I had a great time at the party. Talk to...\"\n",
      "Prediction: âœ… HAM\n",
      "Confidence: Ham 100.00% | Spam 0.00%\n",
      "Risk Level: ğŸŸ¢ LOW RISK\n",
      "\n",
      "ğŸ“¨ Email 4:\n",
      "Content: \"Congratulations! You've won $1000000! Send your bank details immediately to clai...\"\n",
      "Prediction: ğŸš« SPAM\n",
      "Confidence: Ham 0.00% | Spam 100.00%\n",
      "Risk Level: ğŸ”´ HIGH RISK\n",
      "\n",
      "ğŸ“Š Prediction Summary:\n",
      "âœ… Ham emails: 2/4\n",
      "ğŸš« Spam emails: 2/4\n"
     ]
    }
   ],
   "source": [
    "emails = [\n",
    "    'Hey mohan, can we get together to watch footbal game tomorrow?',\n",
    "    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'\n",
    "]\n",
    "emails_count = v.transform(emails) # type: ignore\n",
    "model.predict(emails_count) # type: ignore\n",
    "\n",
    "# Testing model predictions on sample emails\n",
    "print(\"ğŸ”® TESTING MODEL PREDICTIONS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create diverse test emails to evaluate model performance\n",
    "test_emails = [\n",
    "    # Legitimate email (should be classified as Ham - 0)\n",
    "    \"Hey John, can we get together to watch the football game tomorrow? Let me know what time works for you!\",\n",
    "    \n",
    "    # Promotional email (likely to be classified as Spam - 1)\n",
    "    \"URGENT! Up to 50% discount on luxury watches! Exclusive offer just for you. Don't miss this amazing deal! Click now!\",\n",
    "    \n",
    "    # Another legitimate email (should be Ham - 0)\n",
    "    \"Hi Mom, thanks for the birthday wishes. I had a great time at the party. Talk to you soon!\",\n",
    "    \n",
    "    # Suspicious email (likely Spam - 1)\n",
    "    \"Congratulations! You've won $1000000! Send your bank details immediately to claim your prize!\"\n",
    "]\n",
    "\n",
    "print(f\"ğŸ“§ Testing {len(test_emails)} sample emails:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Transform test emails using the same vectorizer\n",
    "test_emails_vectorized = vectorizer.transform(test_emails)\n",
    "\n",
    "# Make predictions\n",
    "predictions = naive_bayes_model.predict(test_emails_vectorized)\n",
    "prediction_probabilities = naive_bayes_model.predict_proba(test_emails_vectorized)\n",
    "\n",
    "# Display results for each email\n",
    "for i, (email, prediction, probabilities) in enumerate(zip(test_emails, predictions, prediction_probabilities)):\n",
    "    print(f\"\\nğŸ“¨ Email {i+1}:\")\n",
    "    print(f\"Content: \\\"{email[:80]}{'...' if len(email) > 80 else ''}\\\"\")\n",
    "    \n",
    "    # Prediction result\n",
    "    classification = \"ğŸš« SPAM\" if prediction == 1 else \"âœ… HAM\"\n",
    "    confidence_spam = probabilities[1] * 100\n",
    "    confidence_ham = probabilities[0] * 100\n",
    "    \n",
    "    print(f\"Prediction: {classification}\")\n",
    "    print(f\"Confidence: Ham {confidence_ham:.2f}% | Spam {confidence_spam:.2f}%\")\n",
    "    \n",
    "    # Risk assessment\n",
    "    if confidence_spam > 80:\n",
    "        risk_level = \"ğŸ”´ HIGH RISK\"\n",
    "    elif confidence_spam > 50:\n",
    "        risk_level = \"ğŸŸ¡ MEDIUM RISK\"\n",
    "    else:\n",
    "        risk_level = \"ğŸŸ¢ LOW RISK\"\n",
    "    \n",
    "    print(f\"Risk Level: {risk_level}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Prediction Summary:\")\n",
    "spam_count = sum(predictions)\n",
    "ham_count = len(predictions) - spam_count\n",
    "print(f\"âœ… Ham emails: {ham_count}/{len(test_emails)}\")\n",
    "print(f\"ğŸš« Spam emails: {spam_count}/{len(test_emails)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d998d10",
   "metadata": {},
   "source": [
    "## ğŸ“Š Step 9: Model Performance Evaluation\n",
    "\n",
    "Time to evaluate how well our spam detector performs! We'll test it on unseen data:\n",
    "\n",
    "- **Test Set Accuracy**: Overall performance metric\n",
    "- **Unseen Data**: Model tested on emails it never saw during training\n",
    "- **Performance Score**: Percentage of correct predictions\n",
    "\n",
    "ğŸ¯ **What Makes a Good Score?**\n",
    "- **Above 90%**: Excellent performance\n",
    "- **85-90%**: Good performance\n",
    "- **Below 85%**: Needs improvement\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/3oriO0OEd9QIDdllqo/giphy.gif\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fe0d007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š MODEL PERFORMANCE EVALUATION\n",
      "========================================\n",
      "ğŸ”„ Vectorizing test data...\n",
      "ğŸ¯ Making predictions on test set...\n",
      "âœ… Evaluation completed!\n",
      "â±ï¸  Prediction time: 0.0015 seconds\n",
      "ğŸ§ª Test samples processed: 1,393\n",
      "âš¡ Prediction speed: 938129 samples/second\n",
      "\n",
      "ğŸ¯ PERFORMANCE METRICS\n",
      "=========================\n",
      "ğŸ¯ Test Accuracy: 0.9835 (98.35%)\n",
      "ğŸ“ˆ Performance Level: ğŸŒŸ EXCELLENT\n",
      "\n",
      "ğŸ“Š CONFUSION MATRIX\n",
      "====================\n",
      "True Negatives (Ham correctly classified): 1202\n",
      "False Positives (Ham misclassified as Spam): 4\n",
      "False Negatives (Spam misclassified as Ham): 19\n",
      "True Positives (Spam correctly classified): 168\n",
      "\n",
      "âš ï¸  ERROR ANALYSIS\n",
      "================\n",
      "False Positive Rate: 0.33% (Ham classified as Spam)\n",
      "False Negative Rate: 10.16% (Spam classified as Ham)\n"
     ]
    }
   ],
   "source": [
    "# Import additional evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“Š MODEL PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Transform test data using the fitted vectorizer\n",
    "print(\"ğŸ”„ Vectorizing test data...\")\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Record prediction time for performance analysis\n",
    "print(\"ğŸ¯ Making predictions on test set...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Make predictions on test set\n",
    "y_test_predictions = naive_bayes_model.predict(X_test_vectorized)\n",
    "y_test_probabilities = naive_bayes_model.predict_proba(X_test_vectorized)\n",
    "\n",
    "prediction_time = time.time() - start_time\n",
    "\n",
    "# Calculate accuracy score\n",
    "test_accuracy = accuracy_score(y_test, y_test_predictions)\n",
    "\n",
    "print(f\"âœ… Evaluation completed!\")\n",
    "print(f\"â±ï¸  Prediction time: {prediction_time:.4f} seconds\")\n",
    "print(f\"ğŸ§ª Test samples processed: {len(X_test):,}\")\n",
    "print(f\"âš¡ Prediction speed: {len(X_test)/prediction_time:.0f} samples/second\")\n",
    "\n",
    "# Performance metrics\n",
    "print(f\"\\nğŸ¯ PERFORMANCE METRICS\")\n",
    "print(f\"=\" * 25)\n",
    "print(f\"ğŸ¯ Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Performance interpretation\n",
    "if test_accuracy >= 0.95:\n",
    "    performance_level = \"ğŸŒŸ EXCELLENT\"\n",
    "elif test_accuracy >= 0.90:\n",
    "    performance_level = \"âœ… VERY GOOD\"\n",
    "elif test_accuracy >= 0.85:\n",
    "    performance_level = \"ğŸ‘ GOOD\"\n",
    "elif test_accuracy >= 0.80:\n",
    "    performance_level = \"âš ï¸  FAIR\"\n",
    "else:\n",
    "    performance_level = \"âŒ NEEDS IMPROVEMENT\"\n",
    "\n",
    "print(f\"ğŸ“ˆ Performance Level: {performance_level}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\nğŸ“Š CONFUSION MATRIX\")\n",
    "print(f\"=\" * 20)\n",
    "cm = confusion_matrix(y_test, y_test_predictions)\n",
    "print(f\"True Negatives (Ham correctly classified): {cm[0,0]}\")\n",
    "print(f\"False Positives (Ham misclassified as Spam): {cm[0,1]}\")\n",
    "print(f\"False Negatives (Spam misclassified as Ham): {cm[1,0]}\")\n",
    "print(f\"True Positives (Spam correctly classified): {cm[1,1]}\")\n",
    "\n",
    "# Calculate error rates safely to avoid ZeroDivisionError\n",
    "if (cm[0,0] + cm[0,1]) > 0:\n",
    "    false_positive_rate = cm[0,1] / (cm[0,0] + cm[0,1]) * 100\n",
    "else:\n",
    "    false_positive_rate = 0.0\n",
    "    print(\"âš ï¸  Warning: No actual Ham messages in test set. False Positive Rate set to 0.\")\n",
    "\n",
    "if (cm[1,0] + cm[1,1]) > 0:\n",
    "    false_negative_rate = cm[1,0] / (cm[1,0] + cm[1,1]) * 100\n",
    "else:\n",
    "    false_negative_rate = 0.0\n",
    "    print(\"âš ï¸  Warning: No actual Spam messages in test set. False Negative Rate set to 0.\")\n",
    "\n",
    "print(f\"\\nâš ï¸  ERROR ANALYSIS\")\n",
    "print(f\"=\" * 16)\n",
    "print(f\"False Positive Rate: {false_positive_rate:.2f}% (Ham classified as Spam)\")\n",
    "print(f\"False Negative Rate: {false_negative_rate:.2f}% (Spam classified as Ham)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed2aff8",
   "metadata": {},
   "source": [
    "## ğŸ”§ Step 10: Building a Production Pipeline\n",
    "\n",
    "Let's create a streamlined pipeline for production use! This combines all preprocessing and prediction steps:\n",
    "\n",
    "- **Pipeline Benefits**: Automated workflow from text to prediction\n",
    "- **No Manual Steps**: Handles vectorization and prediction automatically\n",
    "- **Production Ready**: Can be easily deployed and used\n",
    "\n",
    "ğŸš€ **Pipeline Components:**\n",
    "1. **CountVectorizer**: Text â†’ Numerical features\n",
    "2. **MultinomialNB**: Classification algorithm\n",
    "3. **Automated Flow**: Raw text â†’ Prediction\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExMGx6d2FrMjU5d212cDFid2g5czV3ZTdzMHVjNW5ua3kzZHY1OHp0eSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/9CY7PVOdOLZpIJbGH4/giphy.gif\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c038548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ BUILDING PRODUCTION PIPELINE\n",
      "========================================\n",
      "âœ… Pipeline created successfully!\n",
      "\n",
      "ğŸ”§ PIPELINE CONFIGURATION\n",
      "=========================\n",
      "Step 1: text_vectorizer\n",
      "   Component: CountVectorizer\n",
      "   lowercase: True\n",
      "   stop_words: english\n",
      "   max_features: 5000\n",
      "   ngram_range: (1, 2)\n",
      "\n",
      "Step 2: spam_classifier\n",
      "   Component: MultinomialNB\n",
      "   alpha: 1.0\n",
      "   fit_prior: True\n",
      "\n",
      "ğŸš€ PIPELINE BENEFITS:\n",
      "   âœ… Automated preprocessing\n",
      "   âœ… Consistent transformations\n",
      "   âœ… Easy deployment\n",
      "   âœ… Reduced code complexity\n",
      "   âœ… Reproducible results\n",
      "\n",
      "ğŸ¯ Pipeline is ready for training and deployment!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "print(\"ğŸ”§ BUILDING PRODUCTION PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a comprehensive pipeline combining preprocessing and classification\n",
    "spam_detection_pipeline = Pipeline([\n",
    "    # Step 1: Text Vectorization\n",
    "    ('text_vectorizer', CountVectorizer(\n",
    "        lowercase=True,           # Normalize text to lowercase\n",
    "        stop_words='english',     # Remove common English stop words\n",
    "        max_features=5000,        # Limit vocabulary for efficiency\n",
    "        ngram_range=(1, 2),       # Include unigrams and bigrams\n",
    "        min_df=2,                 # Ignore rare terms (< 2 documents)\n",
    "        max_df=0.95              # Ignore very common terms (> 95% documents)\n",
    "    )),\n",
    "    \n",
    "    # Step 2: Machine Learning Classification\n",
    "    ('spam_classifier', MultinomialNB(\n",
    "        alpha=1.0,               # Laplace smoothing\n",
    "        fit_prior=True           # Learn class prior probabilities\n",
    "    ))\n",
    "])\n",
    "\n",
    "print(\"âœ… Pipeline created successfully!\")\n",
    "print(f\"\\nğŸ”§ PIPELINE CONFIGURATION\")\n",
    "print(f\"=\" * 25)\n",
    "\n",
    "# Display pipeline steps\n",
    "for i, (step_name, step_object) in enumerate(spam_detection_pipeline.steps, 1):\n",
    "    print(f\"Step {i}: {step_name}\")\n",
    "    print(f\"   Component: {step_object.__class__.__name__}\")\n",
    "    \n",
    "    # Display key parameters for each step\n",
    "    if hasattr(step_object, 'get_params'):\n",
    "        key_params = step_object.get_params()\n",
    "        important_params = ['lowercase', 'stop_words', 'max_features', 'ngram_range', 'alpha', 'fit_prior']\n",
    "        for param in important_params:\n",
    "            if param in key_params:\n",
    "                print(f\"   {param}: {key_params[param]}\")\n",
    "    print()\n",
    "\n",
    "print(\"ğŸš€ PIPELINE BENEFITS:\")\n",
    "print(\"   âœ… Automated preprocessing\")\n",
    "print(\"   âœ… Consistent transformations\") \n",
    "print(\"   âœ… Easy deployment\")\n",
    "print(\"   âœ… Reduced code complexity\")\n",
    "print(\"   âœ… Reproducible results\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Pipeline is ready for training and deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b191b7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‹ï¸  TRAINING PRODUCTION PIPELINE\n",
      "========================================\n",
      "ğŸ“Š Training dataset information:\n",
      "   Total samples: 4,179\n",
      "   Ham messages: 3,619\n",
      "   Spam messages: 560\n",
      "   Class balance: 13.40% spam\n",
      "\n",
      "ğŸ”„ Training pipeline components...\n",
      "âœ… Pipeline training completed!\n",
      "â±ï¸  Total training time: 0.2160 seconds\n",
      "\n",
      "ğŸ” PIPELINE VALIDATION\n",
      "======================\n",
      "ğŸ“ Text Vectorizer Status:\n",
      "   Vocabulary size: 5,000 words\n",
      "   Feature matrix shape: 4,179 Ã— 5,000\n",
      "\n",
      "ğŸ¤– Classifier Status:\n",
      "   Algorithm: MultinomialNB\n",
      "   Classes learned: [0 1]\n",
      "   Features processed: 5,000\n",
      "\n",
      "ğŸ§ª Quick validation test:\n",
      "   Sample input: 'This is a test message'\n",
      "   Pipeline prediction: Ham\n",
      "\n",
      "ğŸ¯ Production pipeline is ready for deployment!\n"
     ]
    }
   ],
   "source": [
    "# Train the complete pipeline on raw text data\n",
    "import time\n",
    "\n",
    "print(\"ğŸ‹ï¸  TRAINING PRODUCTION PIPELINE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Record training metrics\n",
    "print(f\"ğŸ“Š Training dataset information:\")\n",
    "print(f\"   Total samples: {len(X_train):,}\")\n",
    "print(f\"   Ham messages: {(y_train == 0).sum():,}\")\n",
    "print(f\"   Spam messages: {(y_train == 1).sum():,}\")\n",
    "print(f\"   Class balance: {((y_train == 1).sum() / len(y_train) * 100):.2f}% spam\")\n",
    "\n",
    "# Train the pipeline\n",
    "print(f\"\\nğŸ”„ Training pipeline components...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Fit the entire pipeline (vectorizer + classifier) on training data\n",
    "spam_detection_pipeline.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"âœ… Pipeline training completed!\")\n",
    "print(f\"â±ï¸  Total training time: {training_time:.4f} seconds\")\n",
    "\n",
    "# Validate pipeline components\n",
    "print(f\"\\nğŸ” PIPELINE VALIDATION\")\n",
    "print(f\"=\" * 22)\n",
    "\n",
    "# Check vectorizer statistics\n",
    "vectorizer_component = spam_detection_pipeline.named_steps['text_vectorizer']\n",
    "classifier_component = spam_detection_pipeline.named_steps['spam_classifier']\n",
    "\n",
    "print(f\"ğŸ“ Text Vectorizer Status:\")\n",
    "print(f\"   Vocabulary size: {len(vectorizer_component.vocabulary_):,} words\")\n",
    "print(f\"   Feature matrix shape: {len(X_train):,} Ã— {len(vectorizer_component.vocabulary_):,}\")\n",
    "\n",
    "print(f\"\\nğŸ¤– Classifier Status:\")\n",
    "print(f\"   Algorithm: {classifier_component.__class__.__name__}\")\n",
    "print(f\"   Classes learned: {classifier_component.classes_}\")\n",
    "print(f\"   Features processed: {classifier_component.feature_count_.shape[1]:,}\")\n",
    "\n",
    "# Test pipeline with a quick sample\n",
    "sample_text = [\"This is a test message\"]\n",
    "sample_prediction = spam_detection_pipeline.predict(sample_text)\n",
    "print(f\"\\nğŸ§ª Quick validation test:\")\n",
    "print(f\"   Sample input: '{sample_text[0]}'\")\n",
    "print(f\"   Pipeline prediction: {'Spam' if sample_prediction[0] == 1 else 'Ham'}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Production pipeline is ready for deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e0a204",
   "metadata": {},
   "source": [
    "## âœ… Step 11: Final Pipeline Testing & Validation\n",
    "\n",
    "Let's verify our production pipeline works perfectly! We'll test both performance and predictions:\n",
    "\n",
    "- **Pipeline Accuracy**: Compare with individual model performance\n",
    "- **Consistency Check**: Ensure same results as step-by-step approach\n",
    "- **Final Validation**: Confirm our system is ready for deployment\n",
    "\n",
    "ğŸ” **Final Checks:**\n",
    "- Same accuracy as manual approach? âœ“\n",
    "- Predictions match previous results? âœ“\n",
    "- Ready for real-world use? âœ“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1bd802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š PIPELINE PERFORMANCE EVALUATION\n",
      "=============================================\n",
      "ğŸ¯ Pipeline Accuracy: 0.9835 (98.35%)\n",
      "â±ï¸  Evaluation time: 0.0370 seconds\n",
      "\n",
      "ğŸ“ˆ PERFORMANCE COMPARISON\n",
      "===========================\n",
      "Individual Model Accuracy: 0.9835 (98.35%)\n",
      "Pipeline Accuracy:         0.9835 (98.35%)\n",
      "Accuracy Difference:       0.0000\n",
      "Consistency Check:         âœ… CONSISTENT\n",
      "\n",
      "âš¡ PERFORMANCE BENCHMARKS\n",
      "===========================\n",
      "Test samples processed: 1,393\n",
      "Processing speed: 37645 emails/second\n",
      "Average time per email: 0.03 milliseconds\n",
      "\n",
      "ğŸ’¾ EFFICIENCY BENEFITS\n",
      "======================\n",
      "âœ… Single object handles entire workflow\n",
      "âœ… Automatic preprocessing pipeline\n",
      "âœ… Consistent feature transformation\n",
      "âœ… Memory-efficient sparse matrices\n",
      "âœ… Ready for production deployment\n",
      "\n",
      "ğŸ¯ Deployment Status: ğŸš€ PRODUCTION READY\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9834888729361091"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate pipeline performance on test data\n",
    "import time\n",
    "\n",
    "print(\"ğŸ“Š PIPELINE PERFORMANCE EVALUATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Record evaluation metrics\n",
    "start_time = time.time()\n",
    "\n",
    "# Evaluate pipeline accuracy on test set\n",
    "pipeline_accuracy = spam_detection_pipeline.score(X_test, y_test)\n",
    "\n",
    "evaluation_time = time.time() - start_time\n",
    "\n",
    "print(f\"ğŸ¯ Pipeline Accuracy: {pipeline_accuracy:.4f} ({pipeline_accuracy*100:.2f}%)\")\n",
    "print(f\"â±ï¸  Evaluation time: {evaluation_time:.4f} seconds\")\n",
    "\n",
    "# Compare with individual model performance\n",
    "print(f\"\\nğŸ“ˆ PERFORMANCE COMPARISON\")\n",
    "print(f\"=\" * 27)\n",
    "print(f\"Individual Model Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(f\"Pipeline Accuracy:         {pipeline_accuracy:.4f} ({pipeline_accuracy*100:.2f}%)\")\n",
    "\n",
    "# Check consistency\n",
    "accuracy_difference = abs(test_accuracy - pipeline_accuracy)\n",
    "consistency_check = \"âœ… CONSISTENT\" if accuracy_difference < 0.001 else \"âš ï¸  DIFFERENT\"\n",
    "\n",
    "print(f\"Accuracy Difference:       {accuracy_difference:.4f}\")\n",
    "print(f\"Consistency Check:         {consistency_check}\")\n",
    "\n",
    "# Performance benchmarking\n",
    "test_samples = len(X_test)\n",
    "throughput = test_samples / evaluation_time\n",
    "\n",
    "print(f\"\\nâš¡ PERFORMANCE BENCHMARKS\")\n",
    "print(f\"=\" * 27)\n",
    "print(f\"Test samples processed: {test_samples:,}\")\n",
    "print(f\"Processing speed: {throughput:.0f} emails/second\")\n",
    "print(f\"Average time per email: {(evaluation_time/test_samples)*1000:.2f} milliseconds\")\n",
    "\n",
    "# Memory efficiency note\n",
    "print(f\"\\nğŸ’¾ EFFICIENCY BENEFITS\")\n",
    "print(f\"=\" * 22)\n",
    "print(\"âœ… Single object handles entire workflow\")\n",
    "print(\"âœ… Automatic preprocessing pipeline\")\n",
    "print(\"âœ… Consistent feature transformation\")\n",
    "print(\"âœ… Memory-efficient sparse matrices\")\n",
    "print(\"âœ… Ready for production deployment\")\n",
    "\n",
    "# Final readiness check\n",
    "if pipeline_accuracy >= 0.90 and consistency_check == \"âœ… CONSISTENT\":\n",
    "    readiness_status = \"ğŸš€ PRODUCTION READY\"\n",
    "elif pipeline_accuracy >= 0.85:\n",
    "    readiness_status = \"âš ï¸  NEEDS MINOR IMPROVEMENTS\"\n",
    "else:\n",
    "    readiness_status = \"âŒ REQUIRES OPTIMIZATION\"\n",
    "\n",
    "print(f\"\\nğŸ¯ Deployment Status: {readiness_status}\")\n",
    "\n",
    "pipeline_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb90cb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final comprehensive pipeline testing\n",
    "print(\"ğŸ”® FINAL PIPELINE VALIDATION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Use the same test emails for consistency comparison\n",
    "print(\"ğŸ“§ Testing pipeline with sample emails...\")\n",
    "\n",
    "# Make predictions using the complete pipeline\n",
    "pipeline_predictions = spam_detection_pipeline.predict(test_emails)\n",
    "pipeline_probabilities = spam_detection_pipeline.predict_proba(test_emails)\n",
    "\n",
    "print(f\"\\nğŸ“Š PIPELINE PREDICTION RESULTS\")\n",
    "print(f\"=\" * 35)\n",
    "\n",
    "# Display detailed results\n",
    "for i, (email, prediction, probabilities) in enumerate(zip(test_emails, pipeline_predictions, pipeline_probabilities)):\n",
    "    print(f\"\\nğŸ“¨ Test Email {i+1}:\")\n",
    "    print(f\"Content: \\\"{email[:60]}{'...' if len(email) > 60 else ''}\\\"\")\n",
    "    \n",
    "    # Classification result\n",
    "    classification = \"ğŸš« SPAM\" if prediction == 1 else \"âœ… HAM\"\n",
    "    confidence_ham = probabilities[0] * 100\n",
    "    confidence_spam = probabilities[1] * 100\n",
    "    \n",
    "    print(f\"Classification: {classification}\")\n",
    "    print(f\"Confidence Scores:\")\n",
    "    print(f\"  Ham: {confidence_ham:.2f}%\")\n",
    "    print(f\"  Spam: {confidence_spam:.2f}%\")\n",
    "    \n",
    "    # Confidence level assessment\n",
    "    max_confidence = max(confidence_ham, confidence_spam)\n",
    "    if max_confidence >= 90:\n",
    "        confidence_level = \"ğŸŸ¢ HIGH CONFIDENCE\"\n",
    "    elif max_confidence >= 70:\n",
    "        confidence_level = \"ğŸŸ¡ MEDIUM CONFIDENCE\"\n",
    "    else:\n",
    "        confidence_level = \"ğŸ”´ LOW CONFIDENCE\"\n",
    "    \n",
    "    print(f\"  Confidence Level: {confidence_level}\")\n",
    "\n",
    "# Consistency check with individual model predictions\n",
    "print(f\"\\nğŸ” CONSISTENCY VERIFICATION\")\n",
    "print(f\"=\" * 28)\n",
    "\n",
    "consistency_matches = sum(1 for pred1, pred2 in zip(predictions, pipeline_predictions) if pred1 == pred2)\n",
    "consistency_rate = (consistency_matches / len(predictions)) * 100\n",
    "\n",
    "print(f\"Prediction matches: {consistency_matches}/{len(predictions)}\")\n",
    "print(f\"Consistency rate: {consistency_rate:.1f}%\")\n",
    "\n",
    "if consistency_rate == 100:\n",
    "    print(\"âœ… Perfect consistency between individual and pipeline models!\")\n",
    "elif consistency_rate >= 95:\n",
    "    print(\"âœ… High consistency - minor differences acceptable\")\n",
    "else:\n",
    "    print(\"âš ï¸  Significant differences detected - requires investigation\")\n",
    "\n",
    "# Real-world deployment readiness\n",
    "print(f\"\\nğŸš€ DEPLOYMENT READINESS CHECKLIST\")\n",
    "print(f\"=\" * 38)\n",
    "\n",
    "checklist_items = [\n",
    "    (\"Model Training\", \"âœ… Completed\"),\n",
    "    (\"Performance Validation\", \"âœ… Completed\"),\n",
    "    (\"Pipeline Integration\", \"âœ… Completed\"),\n",
    "    (\"Consistency Testing\", \"âœ… Completed\"),\n",
    "    (\"Sample Predictions\", \"âœ… Completed\"),\n",
    "    (\"Error Handling\", \"âœ… Built-in\"),\n",
    "    (\"Production Format\", \"âœ… Ready\")\n",
    "]\n",
    "\n",
    "for item, status in checklist_items:\n",
    "    print(f\"  {item:<20}: {status}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Final Status: PIPELINE READY FOR PRODUCTION DEPLOYMENT! ğŸš€\")\n",
    "\n",
    "# Return predictions for further analysis if needed\n",
    "pipeline_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f94f5f",
   "metadata": {},
   "source": [
    "## ğŸ‰ Project Summary & Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a spam detection system! Here's what we accomplished:\n",
    "\n",
    "### ğŸ† **Key Achievements:**\n",
    "- âœ… Built a robust spam classification system\n",
    "- âœ… Achieved high accuracy on email classification\n",
    "- âœ… Created a production-ready ML pipeline\n",
    "- âœ… Demonstrated effective text preprocessing techniques\n",
    "\n",
    "### ğŸ“ˆ **Technical Highlights:**\n",
    "- **Algorithm**: Multinomial Naive Bayes\n",
    "- **Feature Engineering**: Count Vectorization (Bag of Words)\n",
    "- **Pipeline**: Automated preprocessing and prediction\n",
    "- **Performance**: High accuracy on test data\n",
    "\n",
    "### ğŸš€ **Next Steps:**\n",
    "- **Deployment**: Deploy the pipeline to a web application\n",
    "- **Improvements**: Try TF-IDF vectorization or other algorithms\n",
    "- **Feature Engineering**: Add more sophisticated text features\n",
    "- **Evaluation**: Implement detailed performance metrics (precision, recall, F1-score)\n",
    "\n",
    "### ğŸ’¡ **Real-World Applications:**\n",
    "- Email service providers\n",
    "- Corporate security systems\n",
    "- Personal email filters\n",
    "- Content moderation systems\n",
    "\n",
    "---\n",
    "\n",
    "**Great job on completing this spam detection project!** ğŸŠ\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://media.giphy.com/media/26u4lOMA8JKSnL9Uk/giphy.gif\" width=\"500\"/>\n",
    "</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
