{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8a656e",
   "metadata": {},
   "source": [
    "# üéØ K-Fold Cross Validation: Professional Implementation\n",
    "\n",
    "## üìä Overview\n",
    "This notebook demonstrates a comprehensive comparison of machine learning models using K-Fold cross validation techniques. We'll explore:\n",
    "\n",
    "- üîÑ **K-Fold Cross Validation Concepts**\n",
    "- üìà **Model Performance Comparison** \n",
    "- ‚öôÔ∏è **Hyperparameter Tuning**\n",
    "- üìã **Professional Results Analysis**\n",
    "\n",
    "### üé™ What You'll Learn:\n",
    "- How to implement manual and automated cross validation\n",
    "- Best practices for model evaluation\n",
    "- Professional code structure and documentation\n",
    "- Statistical significance of model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3cc45b",
   "metadata": {},
   "source": [
    "## üìö Step 1: Import Required Libraries\n",
    "\n",
    "**Purpose:** Import all necessary libraries organized by category for clean code structure.\n",
    "\n",
    "- üî¢ **NumPy & Matplotlib:** For numerical computations and visualizations\n",
    "- ü§ñ **Scikit-learn:** For machine learning algorithms and cross validation tools\n",
    "- üé≤ **Models:** Logistic Regression, SVM, and Random Forest classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06b204de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross Validation Comparison\n",
    "# Professional implementation of K-Fold cross validation for model comparison\n",
    "\n",
    "# Data Science Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scikit-learn Core\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dc87d93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (1797, 64)\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Data Loading and Preparation\n",
    "digits = load_digits()\n",
    "print(f\"Dataset shape: {digits.data.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(digits.target))}\")\n",
    "\n",
    "# Initial train-test split for baseline comparison\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    digits.data, digits.target, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690fd9b1",
   "metadata": {},
   "source": [
    "## üóÉÔ∏è Step 2: Data Loading & Preparation\n",
    "\n",
    "**Objective:** Load the digits dataset and prepare it for model training.\n",
    "\n",
    "- üìã **Dataset:** Handwritten digits (0-9) classification\n",
    "- üîÑ **Train-Test Split:** 70% training, 30% testing\n",
    "- üéØ **Random State:** Fixed for reproducible results\n",
    "\n",
    "![Data Loading](https://media.giphy.com/media/3oKIPnAiaMCws8nOsE/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "baa0af80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Performance with Single Train-Test Split:\n",
      "==================================================\n",
      "Logistic Regression: 0.9704\n",
      "Support Vector Machine: 0.3926\n",
      "Random Forest: 0.9685\n",
      "Support Vector Machine: 0.3926\n",
      "Random Forest: 0.9685\n"
     ]
    }
   ],
   "source": [
    "# Baseline Model Performance (Single Train-Test Split)\n",
    "print(\"Baseline Performance with Single Train-Test Split:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Logistic Regression\n",
    "lr = LogisticRegression(solver='newton-cg', random_state=42)\n",
    "lr_score = lr.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Logistic Regression: {lr_score:.4f}\")\n",
    "\n",
    "# Support Vector Machine\n",
    "svm = SVC(gamma='auto', random_state=42)\n",
    "svm_score = svm.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Support Vector Machine: {svm_score:.4f}\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=40, random_state=42)\n",
    "rf_score = rf.fit(X_train, y_train).score(X_test, y_test)\n",
    "print(f\"Random Forest: {rf_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bebd18",
   "metadata": {},
   "source": [
    "## üéØ Step 3: Baseline Model Performance\n",
    "\n",
    "**Goal:** Establish baseline performance using a single train-test split.\n",
    "\n",
    "- üî´ **Quick Fire:** Test all three models rapidly\n",
    "- üìä **Comparison:** Get initial performance estimates\n",
    "- ‚ö†Ô∏è **Limitation:** Single split may not be representative\n",
    "\n",
    "> üí° **Pro Tip:** This is just the starting point - cross validation will give us more reliable results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13edaa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "K-Fold Cross Validation Concept:\n",
      "========================================\n",
      "K-Fold split demonstration with sample data [1,2,3,4,5,6,7,8,9]:\n",
      "Fold 1: Train indices: [0 2 3 4 6 8], Test indices: [1 5 7]\n",
      "Fold 2: Train indices: [1 3 4 5 6 7], Test indices: [0 2 8]\n",
      "Fold 3: Train indices: [0 1 2 5 7 8], Test indices: [3 4 6]\n"
     ]
    }
   ],
   "source": [
    "# K-Fold Cross Validation Demonstration\n",
    "print(\"\\nK-Fold Cross Validation Concept:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Simple K-Fold example\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "print(\"K-Fold split demonstration with sample data [1,2,3,4,5,6,7,8,9]:\")\n",
    "for fold, (train_index, test_index) in enumerate(kf.split([1,2,3,4,5,6,7,8,9]), 1):\n",
    "    print(f\"Fold {fold}: Train indices: {train_index}, Test indices: {test_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caa73e4",
   "metadata": {},
   "source": [
    "## üîÑ Step 4: K-Fold Cross Validation Concept\n",
    "\n",
    "**Understanding K-Fold:** Learn how data is split into multiple folds for robust evaluation.\n",
    "\n",
    "- üìÇ **K=3 Folds:** Data divided into 3 equal parts\n",
    "- üîÑ **Rotation:** Each fold serves as test set once\n",
    "- üéØ **Result:** 3 different train-test combinations\n",
    "\n",
    "![K-Fold Animation](https://miro.medium.com/v2/resize:fit:1400/1*J2B_bcbd1-s1kfJeHPEEiQ.gif)\n",
    "\n",
    "### üß† Why K-Fold?\n",
    "- ‚úÖ More reliable performance estimates\n",
    "- ‚úÖ Better use of available data\n",
    "- ‚úÖ Reduces impact of data splitting variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08b60dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def get_score(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train a model and return its accuracy score.\n",
    "    \n",
    "    Parameters:\n",
    "    model: sklearn model instance\n",
    "    X_train, X_test: training and testing features\n",
    "    y_train, y_test: training and testing targets\n",
    "    \n",
    "    Returns:\n",
    "    float: accuracy score\n",
    "    \"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.score(X_test, y_test)\n",
    "\n",
    "def display_cv_results(scores, model_name):\n",
    "    \"\"\"Display cross-validation results in a formatted way.\"\"\"\n",
    "    print(f\"{model_name}:\")\n",
    "    print(f\"  Individual scores: {[f'{score:.4f}' for score in scores]}\")\n",
    "    print(f\"  Mean: {np.mean(scores):.4f} (+/- {np.std(scores) * 2:.4f})\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7c0581",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Step 5: Professional Utility Functions\n",
    "\n",
    "**Code Organization:** Create reusable helper functions for clean, maintainable code.\n",
    "\n",
    "- üéØ **get_score():** Train model and return accuracy\n",
    "- üìä **display_cv_results():** Format cross validation results professionally\n",
    "- üìà **Documentation:** Clear docstrings and type hints\n",
    "\n",
    "> üèóÔ∏è **Best Practice:** Well-documented utility functions make code reusable and professional!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "565f5712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual Stratified K-Fold Cross Validation (3 folds):\n",
      "=======================================================\n",
      "Processing Fold 1...\n",
      "Processing Fold 2...\n",
      "Processing Fold 2...\n",
      "Processing Fold 3...\n",
      "Processing Fold 3...\n",
      "\n",
      "Manual Cross Validation Results:\n",
      "-----------------------------------\n",
      "Logistic Regression:\n",
      "  Individual scores: ['0.9616', '0.9783', '0.9633']\n",
      "  Mean: 0.9677 (+/- 0.0150)\n",
      "\n",
      "Support Vector Machine:\n",
      "  Individual scores: ['0.5659', '0.5659', '0.5710']\n",
      "  Mean: 0.5676 (+/- 0.0047)\n",
      "\n",
      "Random Forest:\n",
      "  Individual scores: ['0.9733', '0.9683', '0.9766']\n",
      "  Mean: 0.9727 (+/- 0.0069)\n",
      "\n",
      "\n",
      "Manual Cross Validation Results:\n",
      "-----------------------------------\n",
      "Logistic Regression:\n",
      "  Individual scores: ['0.9616', '0.9783', '0.9633']\n",
      "  Mean: 0.9677 (+/- 0.0150)\n",
      "\n",
      "Support Vector Machine:\n",
      "  Individual scores: ['0.5659', '0.5659', '0.5710']\n",
      "  Mean: 0.5676 (+/- 0.0047)\n",
      "\n",
      "Random Forest:\n",
      "  Individual scores: ['0.9733', '0.9683', '0.9766']\n",
      "  Mean: 0.9727 (+/- 0.0069)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual Stratified K-Fold Cross Validation\n",
    "print(\"Manual Stratified K-Fold Cross Validation (3 folds):\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Initialize stratified K-fold\n",
    "folds = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize score lists\n",
    "scores_logistic = []\n",
    "scores_svm = []\n",
    "scores_rf = []\n",
    "\n",
    "# Perform cross validation\n",
    "for fold, (train_index, test_index) in enumerate(folds.split(digits.data, digits.target), 1):\n",
    "    print(f\"Processing Fold {fold}...\")\n",
    "    \n",
    "    # Split data\n",
    "    X_train_fold = digits.data[train_index]\n",
    "    X_test_fold = digits.data[test_index]\n",
    "    y_train_fold = digits.target[train_index]\n",
    "    y_test_fold = digits.target[test_index]\n",
    "    \n",
    "    # Evaluate models\n",
    "    scores_logistic.append(get_score(\n",
    "        LogisticRegression(solver='newton-cg', random_state=42), \n",
    "        X_train_fold, X_test_fold, y_train_fold, y_test_fold\n",
    "    ))\n",
    "    scores_svm.append(get_score(\n",
    "        SVC(gamma='auto', random_state=42), \n",
    "        X_train_fold, X_test_fold, y_train_fold, y_test_fold\n",
    "    ))\n",
    "    scores_rf.append(get_score(\n",
    "        RandomForestClassifier(n_estimators=40, random_state=42), \n",
    "        X_train_fold, X_test_fold, y_train_fold, y_test_fold\n",
    "    ))\n",
    "\n",
    "print(\"\\nManual Cross Validation Results:\")\n",
    "print(\"-\" * 35)\n",
    "display_cv_results(scores_logistic, \"Logistic Regression\")\n",
    "display_cv_results(scores_svm, \"Support Vector Machine\")\n",
    "display_cv_results(scores_rf, \"Random Forest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dababc99",
   "metadata": {},
   "source": [
    "## üîß Step 6: Manual Stratified K-Fold Implementation\n",
    "\n",
    "**Deep Dive:** Implement cross validation manually to understand the process step-by-step.\n",
    "\n",
    "- üéØ **Stratified K-Fold:** Maintains class distribution in each fold\n",
    "- üîÑ **3 Folds:** Train on 2 folds, test on 1 fold\n",
    "- üìä **All Models:** Test Logistic Regression, SVM, and Random Forest\n",
    "- üí™ **Manual Control:** Full understanding of the validation process\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "  <img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=\"500\" height=\"300\" alt=\"Resized GIF\">\n",
    "</p>\n",
    "\n",
    "\n",
    "### üé™ The Magic Happens Here:\n",
    "Each fold gives us independent performance estimates! üé≠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a17c31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automated Cross Validation with cross_val_score (3 folds):\n",
      "============================================================\n",
      "Logistic Regression:\n",
      "  Individual scores: ['0.9232', '0.9416', '0.9182']\n",
      "  Mean: 0.9277 (+/- 0.0201)\n",
      "\n",
      "Support Vector Machine:\n",
      "  Individual scores: ['0.3806', '0.4107', '0.5125']\n",
      "  Mean: 0.4346 (+/- 0.1129)\n",
      "\n",
      "Support Vector Machine:\n",
      "  Individual scores: ['0.3806', '0.4107', '0.5125']\n",
      "  Mean: 0.4346 (+/- 0.1129)\n",
      "\n",
      "Random Forest:\n",
      "  Individual scores: ['0.9316', '0.9366', '0.9249']\n",
      "  Mean: 0.9310 (+/- 0.0096)\n",
      "\n",
      "Random Forest:\n",
      "  Individual scores: ['0.9316', '0.9366', '0.9249']\n",
      "  Mean: 0.9310 (+/- 0.0096)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Automated Cross Validation using cross_val_score\n",
    "print(\"Automated Cross Validation with cross_val_score (3 folds):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(solver='newton-cg', random_state=42),\n",
    "    'Support Vector Machine': SVC(gamma='auto', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=40, random_state=42)\n",
    "}\n",
    "\n",
    "# Perform automated cross validation\n",
    "cv_results = {}\n",
    "for name, model in models.items():\n",
    "    scores = cross_val_score(model, digits.data, digits.target, cv=3, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    display_cv_results(scores, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a769efc",
   "metadata": {},
   "source": [
    "## ‚ö° Step 7: Automated Cross Validation with Scikit-Learn\n",
    "\n",
    "**Efficiency Boost:** Use scikit-learn's built-in `cross_val_score` for rapid evaluation.\n",
    "\n",
    "- üöÄ **One-Liner:** Powerful cross validation in minimal code\n",
    "- üîÑ **Same Results:** Should match our manual implementation\n",
    "- ‚ö° **Speed:** Optimized and faster execution\n",
    "- üéØ **Industry Standard:** How professionals do it in practice\n",
    "\n",
    "![Automated Process](https://media.giphy.com/media/JIX9t2j0ZTN9S/giphy.gif)\n",
    "\n",
    "### üèÜ Pro Level: Compare with manual results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e8acdfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Hyperparameter Tuning (n_estimators):\n",
      "==================================================\n",
      "n_estimators= 5: Mean accuracy = 0.8787 (+/- 0.0912)\n",
      "n_estimators=20: Mean accuracy = 0.9332 (+/- 0.0658)\n",
      "n_estimators=20: Mean accuracy = 0.9332 (+/- 0.0658)\n",
      "n_estimators=30: Mean accuracy = 0.9421 (+/- 0.0540)\n",
      "n_estimators=30: Mean accuracy = 0.9421 (+/- 0.0540)\n",
      "n_estimators=40: Mean accuracy = 0.9466 (+/- 0.0500)\n",
      "\n",
      "Best n_estimators: 40 with accuracy: 0.9466\n",
      "n_estimators=40: Mean accuracy = 0.9466 (+/- 0.0500)\n",
      "\n",
      "Best n_estimators: 40 with accuracy: 0.9466\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning: Random Forest n_estimators\n",
    "print(\"Random Forest Hyperparameter Tuning (n_estimators):\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test different n_estimators values\n",
    "n_estimators_values = [5, 20, 30, 40]\n",
    "rf_tuning_results = {}\n",
    "\n",
    "for n_est in n_estimators_values:\n",
    "    model = RandomForestClassifier(n_estimators=n_est, random_state=42)\n",
    "    scores = cross_val_score(model, digits.data, digits.target, cv=10, scoring='accuracy')\n",
    "    mean_score = np.mean(scores)\n",
    "    rf_tuning_results[n_est] = {\n",
    "        'scores': scores,\n",
    "        'mean': mean_score,\n",
    "        'std': np.std(scores)\n",
    "    }\n",
    "    print(f\"n_estimators={n_est:2d}: Mean accuracy = {mean_score:.4f} (+/- {np.std(scores) * 2:.4f})\")\n",
    "\n",
    "# Find best parameter\n",
    "best_n_estimators = max(rf_tuning_results.keys(), key=lambda x: rf_tuning_results[x]['mean'])\n",
    "print(f\"\\nBest n_estimators: {best_n_estimators} with accuracy: {rf_tuning_results[best_n_estimators]['mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34fce93",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Step 8: Hyperparameter Tuning with Cross Validation\n",
    "\n",
    "**Optimization:** Find the best parameters using systematic cross validation approach.\n",
    "\n",
    "- üî´ **Target:** Random Forest `n_estimators` parameter\n",
    "- üéØ **Values Tested:** [5, 20, 30, 40] trees\n",
    "- üìä **10-Fold CV:** More robust evaluation with 10 folds\n",
    "- üèÜ **Best Performance:** Data-driven parameter selection\n",
    "\n",
    "![Parameter Tuning](https://media.giphy.com/media/v1.Y2lkPTc5MGI3NjExdW4zNjloNTMwZnVnamN5bmQ0djExcWFybXZ0OTA5MXo0NDNoMDE2dSZlcD12MV9naWZzX3NlYXJjaCZjdD1n/o6S51npJYQM48/giphy.gif)\n",
    "\n",
    "### üé™ The Gun Show: üí™\n",
    "Watch as we systematically find the optimal number of trees! üå≥üå≥üå≥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "95bfb919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "COMPREHENSIVE MODEL PERFORMANCE SUMMARY\n",
      "============================================================\n",
      "\n",
      "1. BASELINE PERFORMANCE (Single Train-Test Split):\n",
      "   - Shows performance on a single random split\n",
      "   - May not be representative due to variance in data splits\n",
      "\n",
      "2. CROSS VALIDATION RESULTS:\n",
      "   - More reliable performance estimates\n",
      "   - Reduces variance by averaging over multiple folds\n",
      "   - Helps detect overfitting and model stability\n",
      "\n",
      "3. HYPERPARAMETER TUNING:\n",
      "   - Systematic approach to find optimal parameters\n",
      "   - Uses cross validation to avoid overfitting\n",
      "   - Demonstrates the impact of model complexity\n",
      "\n",
      "4. KEY INSIGHTS:\n",
      "   - Cross validation provides more robust performance estimates\n",
      "   - Model ranking may change between single split and CV\n",
      "   - Proper hyperparameter tuning can significantly improve performance\n",
      "   - Random Forest shows sensitivity to n_estimators parameter\n",
      "\n",
      "RECOMMENDATIONS:\n",
      "- Always use cross validation for model evaluation\n",
      "- Consider stratified K-fold for classification tasks\n",
      "- Tune hyperparameters using nested cross validation\n",
      "- Report mean and standard deviation of CV scores\n"
     ]
    }
   ],
   "source": [
    "# Summary and Model Comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n1. BASELINE PERFORMANCE (Single Train-Test Split):\")\n",
    "print(\"   - Shows performance on a single random split\")\n",
    "print(\"   - May not be representative due to variance in data splits\")\n",
    "\n",
    "print(\"\\n2. CROSS VALIDATION RESULTS:\")\n",
    "print(\"   - More reliable performance estimates\")\n",
    "print(\"   - Reduces variance by averaging over multiple folds\")\n",
    "print(\"   - Helps detect overfitting and model stability\")\n",
    "\n",
    "print(\"\\n3. HYPERPARAMETER TUNING:\")\n",
    "print(\"   - Systematic approach to find optimal parameters\")\n",
    "print(\"   - Uses cross validation to avoid overfitting\")\n",
    "print(\"   - Demonstrates the impact of model complexity\")\n",
    "\n",
    "print(\"\\n4. KEY INSIGHTS:\")\n",
    "print(\"   - Cross validation provides more robust performance estimates\")\n",
    "print(\"   - Model ranking may change between single split and CV\")\n",
    "print(\"   - Proper hyperparameter tuning can significantly improve performance\")\n",
    "print(\"   - Random Forest shows sensitivity to n_estimators parameter\")\n",
    "\n",
    "print(\"\\nRECOMMENDATIONS:\")\n",
    "print(\"- Always use cross validation for model evaluation\")\n",
    "print(\"- Consider stratified K-fold for classification tasks\")\n",
    "print(\"- Tune hyperparameters using nested cross validation\")\n",
    "print(\"- Report mean and standard deviation of CV scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2174ca0b",
   "metadata": {},
   "source": [
    "## üèÅ Conclusion & Next Steps\n",
    "\n",
    "### üéØ What We Accomplished:\n",
    "- ‚úÖ **Professional Implementation** of K-Fold cross validation\n",
    "- ‚úÖ **Manual vs Automated** comparison and understanding\n",
    "- ‚úÖ **Hyperparameter Tuning** with systematic approach\n",
    "- ‚úÖ **Best Practices** for model evaluation\n",
    "\n",
    "### üöÄ Next Level Actions:\n",
    "1. **Grid Search CV** for comprehensive hyperparameter tuning\n",
    "2. **Nested Cross Validation** for unbiased performance estimates  \n",
    "3. **Pipeline Integration** for preprocessing and model training\n",
    "4. **Statistical Testing** for significance of performance differences\n",
    "\n",
    "---\n",
    "\n",
    "### üí™ The Gun Show Results: üî´\n",
    "**You've just witnessed professional-grade machine learning evaluation!**\n",
    "\n",
    "![Mission Accomplished](https://media.giphy.com/media/3ohzdIuqJoo8QdKlnW/giphy.gif)\n",
    "\n",
    "---\n",
    "*üìà Keep practicing, keep improving! The journey to ML mastery continues...* üéØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e31feca",
   "metadata": {},
   "source": [
    "## üéä Step 9: Comprehensive Analysis & Recommendations\n",
    "\n",
    "**Final Showdown:** Professional summary of all findings and best practices.\n",
    "\n",
    "- üìä **Performance Comparison:** Baseline vs Cross Validation\n",
    "- üéØ **Key Insights:** What we learned from the analysis\n",
    "- üí° **Best Practices:** Professional recommendations\n",
    "- üèÜ **Winner Selection:** Data-driven model choice\n",
    "\n",
    "![Success](https://media.giphy.com/media/26u4cqiYI30juCOGY/giphy.gif)\n",
    "\n",
    "### üî´ The Final Gun: \n",
    "Time to deliver the knockout punch with professional insights! üí•"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
